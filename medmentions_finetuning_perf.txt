Started fine-tuning of BioBERT on MedMentions

Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1:
	training loss at 0 steps: 1.1278444528579712
	training loss at 100 steps: 25.065439149737358
	training loss at 200 steps: 33.10992904007435
	training loss at 300 steps: 39.443611146882176
	training loss at 400 steps: 44.92420806735754
	training loss at 500 steps: 49.68686235137284
	training loss at 600 steps: 54.508341524749994
	training loss at 700 steps: 59.19593555293977
	training loss at 800 steps: 64.20094596222043
	training loss at 900 steps: 68.93162850290537
	training loss at 1000 steps: 73.5951396804303
	training loss at 1100 steps: 78.13859641551971
	training loss at 1200 steps: 82.60347867012024
	training loss at 1300 steps: 87.37289628759027
	training loss at 1400 steps: 92.15454564243555
	training loss at 1500 steps: 96.48769517987967
	training loss at 1600 steps: 100.72701460123062
	training loss at 1700 steps: 105.1551814712584
	training loss at 1800 steps: 109.07065980322659
	training loss at 1900 steps: 113.30162059981376
	training loss at 2000 steps: 117.37888953927904
	training loss at 2100 steps: 121.49352193903178
	training loss at 2200 steps: 125.60209058970213
	training loss at 2300 steps: 129.83959255181253
	training loss at 2400 steps: 133.90117046795785
	training loss at 2500 steps: 138.20589607395232
	training loss at 2600 steps: 142.2235809341073
	training loss at 2700 steps: 146.364820712246
	training loss at 2800 steps: 150.36461946647614
	training loss at 2900 steps: 154.69623086135834
	training loss at 3000 steps: 158.92938458453864
	training loss at 3100 steps: 163.24528484977782
	training loss at 3200 steps: 167.60658072214574
	training loss at 3300 steps: 171.5142455296591
	training loss at 3400 steps: 175.75483431946486
	training loss at 3500 steps: 179.80582673475146
	training loss at 3600 steps: 183.85236091725528
	training loss at 3700 steps: 187.92309645283967
	training loss at 3800 steps: 191.87081681285053
	training loss at 3900 steps: 195.9503122996539
	training loss at 4000 steps: 199.95231617614627
	training loss at 4100 steps: 203.59006853587925
	training loss at 4200 steps: 207.59206664655358
	training loss at 4300 steps: 211.4982434520498
	training loss at 4400 steps: 215.6500335680321
	training loss at 4500 steps: 219.53716666530818
	training loss at 4600 steps: 223.36343843955547
	training loss at 4700 steps: 227.51766545977443
	training loss at 4800 steps: 231.24277088418603
	Training loss for the epoch: 233.93999749794602
	Training accuracy for epoch: 0.8434675160407371
	Validation loss: 66.31624806579202
Epoch 2:
	training loss at 0 steps: 0.02592800185084343
	training loss at 100 steps: 3.612934160977602
	training loss at 200 steps: 7.062125316821039
	training loss at 300 steps: 10.749318595975637
	training loss at 400 steps: 14.246487406082451
	training loss at 500 steps: 17.77448538504541
	training loss at 600 steps: 21.425097128376365
	training loss at 700 steps: 25.258512772619724
	training loss at 800 steps: 28.540860921144485
	training loss at 900 steps: 32.285609380342066
	training loss at 1000 steps: 35.99975093174726
	training loss at 1100 steps: 39.631690620444715
	training loss at 1200 steps: 43.269219257868826
	training loss at 1300 steps: 47.26359756756574
	training loss at 1400 steps: 50.65921105444431
	training loss at 1500 steps: 54.244591548107564
	training loss at 1600 steps: 57.81685176026076
	training loss at 1700 steps: 61.556002856232226
	training loss at 1800 steps: 64.95215489715338
	training loss at 1900 steps: 68.64934044331312
	training loss at 2000 steps: 72.12690189294517
	training loss at 2100 steps: 75.62399598862976
	training loss at 2200 steps: 79.27220320142806
	training loss at 2300 steps: 82.68003363441676
	training loss at 2400 steps: 86.47420140355825
	training loss at 2500 steps: 90.18644487578422
	training loss at 2600 steps: 93.81429417431355
	training loss at 2700 steps: 97.52195538301021
	training loss at 2800 steps: 100.93069247249514
	training loss at 2900 steps: 104.37315807677805
	training loss at 3000 steps: 107.94599589705467
	training loss at 3100 steps: 111.42639436293393
	training loss at 3200 steps: 115.15154334157705
	training loss at 3300 steps: 118.78632904589176
	training loss at 3400 steps: 122.1680107768625
	training loss at 3500 steps: 125.69486377108842
	training loss at 3600 steps: 129.27396914269775
	training loss at 3700 steps: 132.59717478323728
	training loss at 3800 steps: 136.17578289378434
	training loss at 3900 steps: 139.41145098861307
	training loss at 4000 steps: 142.67536616884172
	training loss at 4100 steps: 145.86673878505826
	training loss at 4200 steps: 149.6821537707001
	training loss at 4300 steps: 153.30237301811576
	training loss at 4400 steps: 156.43206240516156
	training loss at 4500 steps: 160.08199798315763
	training loss at 4600 steps: 163.897903079167
	training loss at 4700 steps: 167.34251133352518
	training loss at 4800 steps: 170.5270828055218
	Training loss for the epoch: 173.15527618397027
	Training accuracy for epoch: 0.8770644967541545
	Validation loss: 67.18945572525263
Epoch 3:
	training loss at 0 steps: 0.028606804087758064
	training loss at 100 steps: 3.0474801217205822
	training loss at 200 steps: 6.046687361318618
	training loss at 300 steps: 8.822501953225583
	training loss at 400 steps: 11.828850886318833
	training loss at 500 steps: 14.739167829509825
	training loss at 600 steps: 17.583888752851635
	training loss at 700 steps: 20.452822390478104
	training loss at 800 steps: 23.298281452152878
	training loss at 900 steps: 26.17391637014225
	training loss at 1000 steps: 28.972437088843435
	training loss at 1100 steps: 31.907940508332103
	training loss at 1200 steps: 34.71020862320438
	training loss at 1300 steps: 37.3239233228378
	training loss at 1400 steps: 40.374594767112285
	training loss at 1500 steps: 43.1956275482662
	training loss at 1600 steps: 45.75481765251607
	training loss at 1700 steps: 48.501270049251616
	training loss at 1800 steps: 51.081848041154444
	training loss at 1900 steps: 54.00487290788442
	training loss at 2000 steps: 56.71140677202493
	training loss at 2100 steps: 59.53353144787252
	training loss at 2200 steps: 62.21812828630209
	training loss at 2300 steps: 64.79194113146514
	training loss at 2400 steps: 67.75109646841884
	training loss at 2500 steps: 70.6957713868469
	training loss at 2600 steps: 73.71769533120096
	training loss at 2700 steps: 76.72745441831648
	training loss at 2800 steps: 79.60219825897366
	training loss at 2900 steps: 82.5452572684735
	training loss at 3000 steps: 85.25209720339626
	training loss at 3100 steps: 87.90988232940435
	training loss at 3200 steps: 90.75383970793337
	training loss at 3300 steps: 93.59071480575949
	training loss at 3400 steps: 96.74924953840673
	training loss at 3500 steps: 99.47287005186081
	training loss at 3600 steps: 102.11406691092998
	training loss at 3700 steps: 105.06406915839761
	training loss at 3800 steps: 108.0053130807355
	training loss at 3900 steps: 110.93445350602269
	training loss at 4000 steps: 113.8554223747924
	training loss at 4100 steps: 116.45625841803849
	training loss at 4200 steps: 119.37821863405406
	training loss at 4300 steps: 122.11348669137806
	training loss at 4400 steps: 124.91557080112398
	training loss at 4500 steps: 127.77935056108981
	training loss at 4600 steps: 130.50839281408116
	training loss at 4700 steps: 133.3340668384917
	training loss at 4800 steps: 136.22769398475066
	Training loss for the epoch: 138.28773049172014
	Training accuracy for epoch: 0.9005594137560623
	Validation loss: 72.58732771174982
Epoch 4:
	training loss at 0 steps: 0.021947402507066727
	training loss at 100 steps: 2.2350107934325933
	training loss at 200 steps: 4.494468749966472
	training loss at 300 steps: 6.974419435020536
	training loss at 400 steps: 9.283798487856984
	training loss at 500 steps: 11.611423714086413
	training loss at 600 steps: 13.826408917084336
	training loss at 700 steps: 15.999427454546094
	training loss at 800 steps: 18.229522231500596
	training loss at 900 steps: 20.439700102899224
	training loss at 1000 steps: 22.719667121767998
	training loss at 1100 steps: 25.038301115855575
	training loss at 1200 steps: 27.205777580849826
	training loss at 1300 steps: 29.55145625816658
	training loss at 1400 steps: 31.648567107971758
	training loss at 1500 steps: 33.846771847456694
	training loss at 1600 steps: 36.325193617492914
	training loss at 1700 steps: 38.65012925025076
	training loss at 1800 steps: 40.96610833797604
	training loss at 1900 steps: 43.201831109821796
	training loss at 2000 steps: 45.71941577363759
	training loss at 2100 steps: 48.03554103686474
	training loss at 2200 steps: 50.22267383639701
	training loss at 2300 steps: 52.53077051579021
	training loss at 2400 steps: 54.80590285058133
	training loss at 2500 steps: 57.04652715404518
	training loss at 2600 steps: 59.373405501944944
	training loss at 2700 steps: 61.459889522520825
	training loss at 2800 steps: 63.68226396641694
	training loss at 2900 steps: 66.05368227395229
	training loss at 3000 steps: 68.32715657609515
	training loss at 3100 steps: 70.50817923131399
	training loss at 3200 steps: 72.69293035077862
	training loss at 3300 steps: 75.0032755236607
	training loss at 3400 steps: 77.355897864094
	training loss at 3500 steps: 79.64993242104538
	training loss at 3600 steps: 82.01494189095683
	training loss at 3700 steps: 84.28775050980039
	training loss at 3800 steps: 86.54588535311632
	training loss at 3900 steps: 88.79463447700255
	training loss at 4000 steps: 91.18410648941062
	training loss at 4100 steps: 93.43071744241752
	training loss at 4200 steps: 95.74386340589263
	training loss at 4300 steps: 97.92640813509934
	training loss at 4400 steps: 100.43829846731387
	training loss at 4500 steps: 102.70001226081513
	training loss at 4600 steps: 105.0551982212346
	training loss at 4700 steps: 107.30330355535261
	training loss at 4800 steps: 109.5464408013504
	Training loss for the epoch: 111.21652592322789
	Training accuracy for epoch: 0.919744054842439
	Validation loss: 82.19581612525508
Epoch 5:
	training loss at 0 steps: 0.015972493216395378
	training loss at 100 steps: 1.9648751700296998
	training loss at 200 steps: 3.810352500062436
	training loss at 300 steps: 5.675469249486923
	training loss at 400 steps: 7.4901699693873525
	training loss at 500 steps: 9.322910000570118
	training loss at 600 steps: 11.170439870562404
	training loss at 700 steps: 13.015062126796693
	training loss at 800 steps: 14.903820055536926
	training loss at 900 steps: 16.86333475727588
	training loss at 1000 steps: 18.554618099704385
	training loss at 1100 steps: 20.60618931008503
	training loss at 1200 steps: 22.39590792171657
	training loss at 1300 steps: 24.275008062366396
	training loss at 1400 steps: 26.271747726481408
	training loss at 1500 steps: 28.14997922256589
	training loss at 1600 steps: 30.013395950663835
	training loss at 1700 steps: 31.78510564379394
	training loss at 1800 steps: 33.722285659983754
	training loss at 1900 steps: 35.77153431857005
	training loss at 2000 steps: 37.66976654389873
	training loss at 2100 steps: 39.50213429611176
	training loss at 2200 steps: 41.483581020496786
	training loss at 2300 steps: 43.378961813636124
	training loss at 2400 steps: 45.271859189495444
	training loss at 2500 steps: 47.1743762139231
	training loss at 2600 steps: 49.08437185105868
	training loss at 2700 steps: 50.8954395072069
	training loss at 2800 steps: 52.86407479108311
	training loss at 2900 steps: 54.83374934014864
	training loss at 3000 steps: 56.82887298730202
	training loss at 3100 steps: 58.73391227261163
	training loss at 3200 steps: 60.665926860878244
	training loss at 3300 steps: 62.53874231339432
	training loss at 3400 steps: 64.48142457357608
	training loss at 3500 steps: 66.34038762398995
	training loss at 3600 steps: 68.42083477997221
	training loss at 3700 steps: 70.51893658121116
	training loss at 3800 steps: 72.37668606149964
	training loss at 3900 steps: 74.45347239566036
	training loss at 4000 steps: 76.17116107954644
	training loss at 4100 steps: 77.99438789696433
	training loss at 4200 steps: 80.02633844106458
	training loss at 4300 steps: 82.09765371237881
	training loss at 4400 steps: 83.96287953131832
	training loss at 4500 steps: 85.74669966311194
	training loss at 4600 steps: 87.6142056172248
	training loss at 4700 steps: 89.3828080825042
	training loss at 4800 steps: 91.32746389624663
	Training loss for the epoch: 92.69769569695927
	Training accuracy for epoch: 0.9327930807848231
	Validation loss: 91.53453714773059
___________________________________
Early stopped at epoch 5.
Epoch 6:
	training loss at 0 steps: 0.005195619538426399
	training loss at 100 steps: 1.6500137178227305
	training loss at 200 steps: 3.357927691191435
	training loss at 300 steps: 4.886353403329849
	training loss at 400 steps: 6.512270936742425
	training loss at 500 steps: 8.076909807045013
	training loss at 600 steps: 9.681489808484912
	training loss at 700 steps: 11.334174182731658
	training loss at 800 steps: 13.103232220746577
	training loss at 900 steps: 14.672495533712208
	training loss at 1000 steps: 16.25344296870753
	training loss at 1100 steps: 17.976528454804793
	training loss at 1200 steps: 19.601608952973038
	training loss at 1300 steps: 21.12756204744801
	training loss at 1400 steps: 22.790252259001136
	training loss at 1500 steps: 24.271775705739856
	training loss at 1600 steps: 25.934400672093034
	training loss at 1700 steps: 27.572856361977756
	training loss at 1800 steps: 29.176866485038772
	training loss at 1900 steps: 30.85028385487385
	training loss at 2000 steps: 32.515415833564475
	training loss at 2100 steps: 34.16853501438163
	training loss at 2200 steps: 35.90212328010239
	training loss at 2300 steps: 37.43115363433026
	training loss at 2400 steps: 38.97847811318934
	training loss at 2500 steps: 40.60662462213077
	training loss at 2600 steps: 42.201088587986305
	training loss at 2700 steps: 43.76918135304004
	training loss at 2800 steps: 45.364429011475295
	training loss at 2900 steps: 47.070562980370596
	training loss at 3000 steps: 48.606973589165136
	training loss at 3100 steps: 50.189394872868434
	training loss at 3200 steps: 51.81318989000283
	training loss at 3300 steps: 53.45989309879951
	training loss at 3400 steps: 55.172083457699046
	training loss at 3500 steps: 56.769029796356335
	training loss at 3600 steps: 58.43515223241411
	training loss at 3700 steps: 60.08942051860504
	training loss at 3800 steps: 61.772303054342046
	training loss at 3900 steps: 63.31192972068675
	training loss at 4000 steps: 64.97436683508568
	training loss at 4100 steps: 66.5388374121394
	training loss at 4200 steps: 68.13144734944217
	training loss at 4300 steps: 69.70705754705705
	training loss at 4400 steps: 71.34472198062576
	training loss at 4500 steps: 72.98051075614057
	training loss at 4600 steps: 74.58969275490381
	training loss at 4700 steps: 76.14116861135699
	training loss at 4800 steps: 77.83290885644965
	Training loss for the epoch: 79.08586709480733
	Training accuracy for epoch: 0.9425653724060323
	Validation loss: 96.42453296249732
Epoch 7:
	training loss at 0 steps: 0.013650892302393913
	training loss at 100 steps: 1.3029863918200135
	training loss at 200 steps: 2.662627249956131
	training loss at 300 steps: 3.9954561009071767
	training loss at 400 steps: 5.349393758457154
	training loss at 500 steps: 6.698859086725861
	training loss at 600 steps: 8.11460784310475
	training loss at 700 steps: 9.60092466743663
	training loss at 800 steps: 10.967996281571686
	training loss at 900 steps: 12.318848794326186
	training loss at 1000 steps: 13.677440893137828
	training loss at 1100 steps: 15.046628777170554
	training loss at 1200 steps: 16.427871835883707
	training loss at 1300 steps: 17.87617032788694
	training loss at 1400 steps: 19.168029113672674
	training loss at 1500 steps: 20.586822924669832
	training loss at 1600 steps: 21.966721033211797
	training loss at 1700 steps: 23.355184827232733
	training loss at 1800 steps: 24.709073807345703
	training loss at 1900 steps: 26.09020382980816
	training loss at 2000 steps: 27.554381027584895
	training loss at 2100 steps: 28.978018069872633
	training loss at 2200 steps: 30.324205616721883
	training loss at 2300 steps: 31.780599169665948
	training loss at 2400 steps: 33.34408279322088
	training loss at 2500 steps: 34.741722027305514
	training loss at 2600 steps: 36.158993425779045
	training loss at 2700 steps: 37.67400613613427
	training loss at 2800 steps: 39.045539029175416
	training loss at 2900 steps: 40.43375550280325
	training loss at 3000 steps: 41.649037572788075
	training loss at 3100 steps: 43.18001074180938
	training loss at 3200 steps: 44.69353593722917
	training loss at 3300 steps: 46.133398747770116
	training loss at 3400 steps: 47.579840931110084
	training loss at 3500 steps: 48.99929450312629
	training loss at 3600 steps: 50.2630444993265
	training loss at 3700 steps: 51.54498434625566
	training loss at 3800 steps: 52.84705351176672
	training loss at 3900 steps: 54.29675055691041
	training loss at 4000 steps: 55.70538254431449
	training loss at 4100 steps: 57.033129253657535
	training loss at 4200 steps: 58.41387101681903
	training loss at 4300 steps: 59.87110915966332
	training loss at 4400 steps: 61.34244113881141
	training loss at 4500 steps: 62.82076563872397
	training loss at 4600 steps: 64.2444511144422
	training loss at 4700 steps: 65.64364233473316
	training loss at 4800 steps: 67.07478069397621
	Training loss for the epoch: 68.07585081201978
	Training accuracy for epoch: 0.9508078722612315
	Validation loss: 108.52016674214974
Epoch 8:
	training loss at 0 steps: 0.00711118383333087
	training loss at 100 steps: 1.1234634881839156
	training loss at 200 steps: 2.3319602431729436
	training loss at 300 steps: 3.502108631422743
	training loss at 400 steps: 4.72126642614603
	training loss at 500 steps: 5.820319348480552
	training loss at 600 steps: 7.0241084434092045
	training loss at 700 steps: 8.238854394527152
	training loss at 800 steps: 9.317347592674196
	training loss at 900 steps: 10.418866142164916
	training loss at 1000 steps: 11.567350999917835
	training loss at 1100 steps: 12.7310573796276
	training loss at 1200 steps: 13.952626494690776
	training loss at 1300 steps: 15.190728103276342
	training loss at 1400 steps: 16.402346833376214
	training loss at 1500 steps: 17.498470113845542
	training loss at 1600 steps: 18.643454866483808
	training loss at 1700 steps: 19.731866725953296
	training loss at 1800 steps: 20.897884493228048
	training loss at 1900 steps: 22.10204372566659
	training loss at 2000 steps: 23.32414421683643
	training loss at 2100 steps: 24.58433776756283
	training loss at 2200 steps: 25.7655107708415
	training loss at 2300 steps: 26.94425762386527
	training loss at 2400 steps: 28.21137688343879
	training loss at 2500 steps: 29.39293817046564
	training loss at 2600 steps: 30.622142817941494
	training loss at 2700 steps: 31.8017945549218
	training loss at 2800 steps: 32.95815879141446
	training loss at 2900 steps: 34.20339101587888
	training loss at 3000 steps: 35.37410588667262
	training loss at 3100 steps: 36.58816190867219
	training loss at 3200 steps: 37.82789113128092
	training loss at 3300 steps: 39.08325423649512
	training loss at 3400 steps: 40.20813960058149
	training loss at 3500 steps: 41.505514788324945
	training loss at 3600 steps: 42.652992141549475
	training loss at 3700 steps: 43.793863169965334
	training loss at 3800 steps: 45.02661215595435
	training loss at 3900 steps: 46.30466180073563
	training loss at 4000 steps: 47.553179551963694
	training loss at 4100 steps: 48.82216588209849
	training loss at 4200 steps: 49.99028061365243
	training loss at 4300 steps: 51.224981869454496
	training loss at 4400 steps: 52.549485385534354
	training loss at 4500 steps: 53.731229556840844
	training loss at 4600 steps: 54.947048839065246
	training loss at 4700 steps: 56.13659100711811
	training loss at 4800 steps: 57.437892350018956
	Training loss for the epoch: 58.326331634190865
	Training accuracy for epoch: 0.9575712868569133
	Validation loss: 111.70559754781425
Epoch 9:
	training loss at 0 steps: 0.009628276340663433
	training loss at 100 steps: 1.0729662775993347
	training loss at 200 steps: 2.0245009437203407
	training loss at 300 steps: 2.9231299974489957
	training loss at 400 steps: 3.8337066137464717
	training loss at 500 steps: 4.873183068004437
	training loss at 600 steps: 5.920873344526626
	training loss at 700 steps: 6.901478290208615
	training loss at 800 steps: 7.952029304811731
	training loss at 900 steps: 8.942148388363421
	training loss at 1000 steps: 10.08278193604201
	training loss at 1100 steps: 11.035863314406015
	training loss at 1200 steps: 12.060373936081305
	training loss at 1300 steps: 13.069537859875709
	training loss at 1400 steps: 14.178786077536643
	training loss at 1500 steps: 15.27070059766993
	training loss at 1600 steps: 16.252348051173612
	training loss at 1700 steps: 17.270616651279852
	training loss at 1800 steps: 18.30633456283249
	training loss at 1900 steps: 19.397781655658036
	training loss at 2000 steps: 20.32222956698388
	training loss at 2100 steps: 21.40514937741682
	training loss at 2200 steps: 22.42872449127026
	training loss at 2300 steps: 23.428173934808
	training loss at 2400 steps: 24.43825236056
	training loss at 2500 steps: 25.447479462483898
	training loss at 2600 steps: 26.46701184893027
	training loss at 2700 steps: 27.469677872490138
	training loss at 2800 steps: 28.48385743633844
	training loss at 2900 steps: 29.4548215882387
	training loss at 3000 steps: 30.49454380152747
	training loss at 3100 steps: 31.528938698000275
	training loss at 3200 steps: 32.5653698957758
	training loss at 3300 steps: 33.62189312081318
	training loss at 3400 steps: 34.75010543887038
	training loss at 3500 steps: 35.83369720063638
	training loss at 3600 steps: 36.88903531956021
	training loss at 3700 steps: 37.92078511288855
	training loss at 3800 steps: 39.02569610590581
	training loss at 3900 steps: 40.1957828522427
	training loss at 4000 steps: 41.16632212034892
	training loss at 4100 steps: 42.167584814014845
	training loss at 4200 steps: 43.155852242722176
	training loss at 4300 steps: 44.137763217207976
	training loss at 4400 steps: 45.18024430202786
	training loss at 4500 steps: 46.21696083818097
	training loss at 4600 steps: 47.2583838008577
	training loss at 4700 steps: 48.262641700101085
	training loss at 4800 steps: 49.21911313536111
	Training loss for the epoch: 49.955671474919654
	Training accuracy for epoch: 0.9636832980408644
	Validation loss: 124.26953697530553
Epoch 10:
	training loss at 0 steps: 0.008289894089102745
	training loss at 100 steps: 0.843771596904844
	training loss at 200 steps: 1.7500287450384349
	training loss at 300 steps: 2.6195030845701694
	training loss at 400 steps: 3.472946503898129
	training loss at 500 steps: 4.30006149969995
	training loss at 600 steps: 5.124955910723656
	training loss at 700 steps: 6.019494315609336
	training loss at 800 steps: 6.8709982426371425
	training loss at 900 steps: 7.746085366699845
	training loss at 1000 steps: 8.63512728502974
	training loss at 1100 steps: 9.52531230461318
	training loss at 1200 steps: 10.344899178599007
	training loss at 1300 steps: 11.243395952158608
	training loss at 1400 steps: 12.12426366400905
	training loss at 1500 steps: 13.02750133164227
	training loss at 1600 steps: 13.864658669568598
	training loss at 1700 steps: 14.684752409579232
	training loss at 1800 steps: 15.500461867661215
	training loss at 1900 steps: 16.4038881509332
	training loss at 2000 steps: 17.240544195869006
	training loss at 2100 steps: 18.19290860311594
	training loss at 2200 steps: 19.05326922936365
	training loss at 2300 steps: 19.831301510334015
	training loss at 2400 steps: 20.684990330715664
	training loss at 2500 steps: 21.59079895459581
	training loss at 2600 steps: 22.493985236040317
	training loss at 2700 steps: 23.333077618735842
	training loss at 2800 steps: 24.233469792525284
	training loss at 2900 steps: 25.20148569776211
	training loss at 3000 steps: 26.094958709203638
	training loss at 3100 steps: 27.004838380846195
	training loss at 3200 steps: 27.89597637730185
	training loss at 3300 steps: 28.846602039062418
	training loss at 3400 steps: 29.75453781208489
	training loss at 3500 steps: 30.65608978853561
	training loss at 3600 steps: 31.526139053632505
	training loss at 3700 steps: 32.41003081644885
	training loss at 3800 steps: 33.21176075364929
	training loss at 3900 steps: 34.17577970039565
	training loss at 4000 steps: 35.12561070511583
	training loss at 4100 steps: 36.042111002258025
	training loss at 4200 steps: 36.923773503280245
	training loss at 4300 steps: 37.8188256471185
	training loss at 4400 steps: 38.74776792083867
	training loss at 4500 steps: 39.66298374882899
	training loss at 4600 steps: 40.55796625488438
	training loss at 4700 steps: 41.4246415165253
	training loss at 4800 steps: 42.359699483495206
	Training loss for the epoch: 43.060878400690854
	Training accuracy for epoch: 0.9688442999388276
	Validation loss: 131.49408149160445
Epoch 11:
	training loss at 0 steps: 0.009795673191547394
	training loss at 100 steps: 0.7874313758220524
	training loss at 200 steps: 1.5324791697785258
	training loss at 300 steps: 2.268918045796454
	training loss at 400 steps: 2.994169180863537
	training loss at 500 steps: 3.706047931453213
	training loss at 600 steps: 4.3841287231771275
	training loss at 700 steps: 5.0253490773029625
	training loss at 800 steps: 5.770211663329974
	training loss at 900 steps: 6.632919407682493
	training loss at 1000 steps: 7.3581117279827595
	training loss at 1100 steps: 8.098542259307578
	training loss at 1200 steps: 8.782091738306917
	training loss at 1300 steps: 9.440367086674087
	training loss at 1400 steps: 10.144627130473964
	training loss at 1500 steps: 10.93475257360842
	training loss at 1600 steps: 11.703785149962641
	training loss at 1700 steps: 12.443238400272094
	training loss at 1800 steps: 13.176108911633492
	training loss at 1900 steps: 13.923899205052294
	training loss at 2000 steps: 14.597256465698592
	training loss at 2100 steps: 15.287820404046215
	training loss at 2200 steps: 16.12998408381827
	training loss at 2300 steps: 16.918811503681354
	training loss at 2400 steps: 17.676425748621114
	training loss at 2500 steps: 18.392589028575458
	training loss at 2600 steps: 19.090518633369356
	training loss at 2700 steps: 19.85977429302875
	training loss at 2800 steps: 20.66315940779168
	training loss at 2900 steps: 21.54052637773566
	training loss at 3000 steps: 22.354325502179563
	training loss at 3100 steps: 23.20859470916912
	training loss at 3200 steps: 23.995738493744284
	training loss at 3300 steps: 24.70542004611343
	training loss at 3400 steps: 25.428212411236018
	training loss at 3500 steps: 26.218303414294496
	training loss at 3600 steps: 27.02598981920164
	training loss at 3700 steps: 27.776450599660166
	training loss at 3800 steps: 28.49333524069516
	training loss at 3900 steps: 29.23437553603435
	training loss at 4000 steps: 29.990828608104493
	training loss at 4100 steps: 30.766534278576728
	training loss at 4200 steps: 31.541778459504712
	training loss at 4300 steps: 32.342235467920545
	training loss at 4400 steps: 33.06787091266597
	training loss at 4500 steps: 33.899688651261386
	training loss at 4600 steps: 34.709121662948746
	training loss at 4700 steps: 35.50282914581476
	training loss at 4800 steps: 36.28642945032334
	Training loss for the epoch: 36.8705929259886
	Training accuracy for epoch: 0.9738090199337109
	Validation loss: 136.2469412900973
Epoch 12:
	training loss at 0 steps: 0.003920354414731264
	training loss at 100 steps: 0.600114434841089
	training loss at 200 steps: 1.20066713943379
	training loss at 300 steps: 1.7871682352852076
	training loss at 400 steps: 2.4324612212367356
	training loss at 500 steps: 3.039001854020171
	training loss at 600 steps: 3.696379779779818
	training loss at 700 steps: 4.307752403838094
	training loss at 800 steps: 4.895805484557059
	training loss at 900 steps: 5.448972816870082
	training loss at 1000 steps: 6.081687656987924
	training loss at 1100 steps: 6.674371596367564
	training loss at 1200 steps: 7.3293721656664275
	training loss at 1300 steps: 7.927529783279169
	training loss at 1400 steps: 8.522911464620847
	training loss at 1500 steps: 9.163037181133404
	training loss at 1600 steps: 9.760535813984461
	training loss at 1700 steps: 10.447781884344295
	training loss at 1800 steps: 11.091603061999194
	training loss at 1900 steps: 11.62571923690848
	training loss at 2000 steps: 12.277675452758558
	training loss at 2100 steps: 12.929904310381971
	training loss at 2200 steps: 13.566273496602662
	training loss at 2300 steps: 14.199180709314533
	training loss at 2400 steps: 14.80089054646669
	training loss at 2500 steps: 15.445287719776388
	training loss at 2600 steps: 16.110723685647827
	training loss at 2700 steps: 16.69446643331321
	training loss at 2800 steps: 17.3288622686523
	training loss at 2900 steps: 18.095944331900682
	training loss at 3000 steps: 18.74101201427402
	training loss at 3100 steps: 19.405054738686886
	training loss at 3200 steps: 20.06165526312543
	training loss at 3300 steps: 20.718827890988905
	training loss at 3400 steps: 21.37484367506113
	training loss at 3500 steps: 21.999579365248792
	training loss at 3600 steps: 22.646026391419582
	training loss at 3700 steps: 23.261607003980316
	training loss at 3800 steps: 23.936925133340992
	training loss at 3900 steps: 24.622265990939923
	training loss at 4000 steps: 25.295012713526376
	training loss at 4100 steps: 26.001009052211884
	training loss at 4200 steps: 26.582194214570336
	training loss at 4300 steps: 27.244923275779
	training loss at 4400 steps: 27.88865901227109
	training loss at 4500 steps: 28.595936111174524
	training loss at 4600 steps: 29.277399197686464
	training loss at 4700 steps: 30.002234080224298
	training loss at 4800 steps: 30.600468455115333
	Training loss for the epoch: 31.113285693805665
	Training accuracy for epoch: 0.9779651430727521
	Validation loss: 150.33233295287937
Epoch 13:
	training loss at 0 steps: 0.005444470793008804
	training loss at 100 steps: 0.5171815926441923
	training loss at 200 steps: 1.0844078658847138
	training loss at 300 steps: 1.6035379862878472
	training loss at 400 steps: 2.1091000778833404
	training loss at 500 steps: 2.6073942384682596
	training loss at 600 steps: 3.197131779626943
	training loss at 700 steps: 3.731990071362816
	training loss at 800 steps: 4.200540980848018
	training loss at 900 steps: 4.69045272807125
	training loss at 1000 steps: 5.211753737472463
	training loss at 1100 steps: 5.758987590088509
	training loss at 1200 steps: 6.309415468771476
	training loss at 1300 steps: 6.8227629871689714
	training loss at 1400 steps: 7.303383687452879
	training loss at 1500 steps: 7.853684922738466
	training loss at 1600 steps: 8.363679376256187
	training loss at 1700 steps: 8.951828336634208
	training loss at 1800 steps: 9.50821880903095
	training loss at 1900 steps: 10.059257549699396
	training loss at 2000 steps: 10.561240307928529
	training loss at 2100 steps: 11.137015257088933
	training loss at 2200 steps: 11.685886253544595
	training loss at 2300 steps: 12.236487736168783
	training loss at 2400 steps: 12.824433740112
	training loss at 2500 steps: 13.4401540257968
	training loss at 2600 steps: 13.990300951816607
	training loss at 2700 steps: 14.526456909719855
	training loss at 2800 steps: 15.132245575892739
	training loss at 2900 steps: 15.668143111048266
	training loss at 3000 steps: 16.177283238852397
	training loss at 3100 steps: 16.738970292091835
	training loss at 3200 steps: 17.28389380738372
	training loss at 3300 steps: 17.802781648759264
	training loss at 3400 steps: 18.369076376664452
	training loss at 3500 steps: 18.98998693563044
	training loss at 3600 steps: 19.574847985990345
	training loss at 3700 steps: 20.188282272429205
	training loss at 3800 steps: 20.756534263375215
	training loss at 3900 steps: 21.246299357968383
	training loss at 4000 steps: 21.774335160676856
	training loss at 4100 steps: 22.329879493976478
	training loss at 4200 steps: 22.886440452828538
	training loss at 4300 steps: 23.540821672941092
	training loss at 4400 steps: 24.08748153288616
	training loss at 4500 steps: 24.619015829171985
	training loss at 4600 steps: 25.190432061848696
	training loss at 4700 steps: 25.80039920535637
	training loss at 4800 steps: 26.361935098597314
	Training loss for the epoch: 26.790354691504035
	Training accuracy for epoch: 0.9810278867165372
	Validation loss: 153.8415092821233
Epoch 14:
	training loss at 0 steps: 0.0030256337486207485
	training loss at 100 steps: 0.44667297234991565
	training loss at 200 steps: 0.8392862803884782
	training loss at 300 steps: 1.2692496586823836
	training loss at 400 steps: 1.681482630170649
	training loss at 500 steps: 2.1653188783384394
	training loss at 600 steps: 2.630433714337414
	training loss at 700 steps: 3.101241479802411
	training loss at 800 steps: 3.5541004018159583
	training loss at 900 steps: 3.9765946630504914
	training loss at 1000 steps: 4.419917635037564
	training loss at 1100 steps: 4.902038010885008
	training loss at 1200 steps: 5.36172794981394
	training loss at 1300 steps: 5.805229018034879
	training loss at 1400 steps: 6.304140307591297
	training loss at 1500 steps: 6.754937739577144
	training loss at 1600 steps: 7.214044311782345
	training loss at 1700 steps: 7.702173015743028
	training loss at 1800 steps: 8.179332418949343
	training loss at 1900 steps: 8.61167149056564
	training loss at 2000 steps: 9.081039313023211
	training loss at 2100 steps: 9.637155056494521
	training loss at 2200 steps: 10.128088157245656
	training loss at 2300 steps: 10.58845391657087
	training loss at 2400 steps: 11.119083681929624
	training loss at 2500 steps: 11.618545912875561
	training loss at 2600 steps: 12.101836432091659
	training loss at 2700 steps: 12.56447878779727
	training loss at 2800 steps: 13.123191175720422
	training loss at 2900 steps: 13.60375699904398
	training loss at 3000 steps: 14.048894419072894
	training loss at 3100 steps: 14.488207452523056
	training loss at 3200 steps: 15.037247797765303
	training loss at 3300 steps: 15.502465266326908
	training loss at 3400 steps: 15.946192771778442
	training loss at 3500 steps: 16.537149824085645
	training loss at 3600 steps: 16.984754794393666
	training loss at 3700 steps: 17.480318195652217
	training loss at 3800 steps: 17.96772924234392
	training loss at 3900 steps: 18.63018609554274
	training loss at 4000 steps: 19.173042228852864
	training loss at 4100 steps: 19.600711470935494
	training loss at 4200 steps: 20.056062912219204
	training loss at 4300 steps: 20.49462419608608
	training loss at 4400 steps: 20.965606903191656
	training loss at 4500 steps: 21.5218491164851
	training loss at 4600 steps: 22.007999487628695
	training loss at 4700 steps: 22.59305784368189
	training loss at 4800 steps: 23.068764508177992
	Training loss for the epoch: 23.434886280971114
	Training accuracy for epoch: 0.9837249304149299
	Validation loss: 159.18081560079008
Epoch 15:
	training loss at 0 steps: 0.01736854575574398
	training loss at 100 steps: 0.43992374558001757
	training loss at 200 steps: 0.8492576190328691
	training loss at 300 steps: 1.2165075711382087
	training loss at 400 steps: 1.5983824942668434
	training loss at 500 steps: 1.9507977744506206
	training loss at 600 steps: 2.337950850807829
	training loss at 700 steps: 2.7396942023769952
	training loss at 800 steps: 3.1638937953393906
	training loss at 900 steps: 3.535750216949964
	training loss at 1000 steps: 3.9411250440753065
	training loss at 1100 steps: 4.352048832923174
	training loss at 1200 steps: 4.813190047163516
	training loss at 1300 steps: 5.225633234193083
	training loss at 1400 steps: 5.627772440726403
	training loss at 1500 steps: 6.0958914780349005
	training loss at 1600 steps: 6.493792337802006
	training loss at 1700 steps: 6.892879976483528
	training loss at 1800 steps: 7.234236237825826
	training loss at 1900 steps: 7.601578860776499
	training loss at 2000 steps: 7.972604697395582
	training loss at 2100 steps: 8.309169642016059
	training loss at 2200 steps: 8.701453182875412
	training loss at 2300 steps: 9.059859340137336
	training loss at 2400 steps: 9.52273287664866
	training loss at 2500 steps: 9.886758836597437
	training loss at 2600 steps: 10.354245795082534
	training loss at 2700 steps: 10.762777879281202
	training loss at 2800 steps: 11.214385720406426
	training loss at 2900 steps: 11.61195543181384
	training loss at 3000 steps: 12.012699571670964
	training loss at 3100 steps: 12.409178533649538
	training loss at 3200 steps: 12.800045106967445
	training loss at 3300 steps: 13.208615281269886
	training loss at 3400 steps: 13.62451468850486
	training loss at 3500 steps: 14.040345162589801
	training loss at 3600 steps: 14.453448133310303
	training loss at 3700 steps: 14.864778767747339
	training loss at 3800 steps: 15.375087891181465
	training loss at 3900 steps: 15.866307539719855
	training loss at 4000 steps: 16.294088020542404
	training loss at 4100 steps: 16.71215257010772
	training loss at 4200 steps: 17.11387736524921
	training loss at 4300 steps: 17.666131847247016
	training loss at 4400 steps: 18.090477656864095
	training loss at 4500 steps: 18.56234634941211
	training loss at 4600 steps: 19.01693620835431
	training loss at 4700 steps: 19.405072238005232
	training loss at 4800 steps: 19.793778995634057
	Training loss for the epoch: 20.066763776820153
	Training accuracy for epoch: 0.9862685931016483
	Validation loss: 163.47686676494777
Epoch 16:
	training loss at 0 steps: 0.0031090618576854467
	training loss at 100 steps: 0.3265417688526213
	training loss at 200 steps: 0.6359611746156588
	training loss at 300 steps: 0.9460257304599509
	training loss at 400 steps: 1.260008857701905
	training loss at 500 steps: 1.63376352461637
	training loss at 600 steps: 1.9419281087757554
	training loss at 700 steps: 2.2730352477519773
	training loss at 800 steps: 2.6019317052850965
	training loss at 900 steps: 2.943358228454599
	training loss at 1000 steps: 3.2365969555685297
	training loss at 1100 steps: 3.58307210370549
	training loss at 1200 steps: 3.924959968047915
	training loss at 1300 steps: 4.275490066269413
	training loss at 1400 steps: 4.592347417055862
	training loss at 1500 steps: 4.928299076942494
	training loss at 1600 steps: 5.296015997650102
	training loss at 1700 steps: 5.704094354849076
	training loss at 1800 steps: 6.052173479591147
	training loss at 1900 steps: 6.419157414653455
	training loss at 2000 steps: 6.73955204806407
	training loss at 2100 steps: 7.015297168458346
	training loss at 2200 steps: 7.394857972041791
	training loss at 2300 steps: 7.740052485805791
	training loss at 2400 steps: 8.101461308160651
	training loss at 2500 steps: 8.448390757952438
	training loss at 2600 steps: 8.812729786815908
	training loss at 2700 steps: 9.182228098386986
	training loss at 2800 steps: 9.524603271329397
	training loss at 2900 steps: 9.885445367744978
	training loss at 3000 steps: 10.276670882125472
	training loss at 3100 steps: 10.62836343404706
	training loss at 3200 steps: 11.057685340896569
	training loss at 3300 steps: 11.428924778560031
	training loss at 3400 steps: 11.766830130680319
	training loss at 3500 steps: 12.09417756937546
	training loss at 3600 steps: 12.476357303788973
	training loss at 3700 steps: 12.76966250384794
	training loss at 3800 steps: 13.086102498749824
	training loss at 3900 steps: 13.400069236431591
	training loss at 4000 steps: 13.752064953583613
	training loss at 4100 steps: 14.146599737836368
	training loss at 4200 steps: 14.508746824019909
	training loss at 4300 steps: 14.887594069496117
	training loss at 4400 steps: 15.244600878151687
	training loss at 4500 steps: 15.57843788264654
	training loss at 4600 steps: 15.96344867764492
	training loss at 4700 steps: 16.276700971993705
	training loss at 4800 steps: 16.641214108149143
	Training loss for the epoch: 16.88898846495067
	Training accuracy for epoch: 0.9883594587875902
	Validation loss: 177.94383811112493
Epoch 17:
	training loss at 0 steps: 0.002450305502861738
	training loss at 100 steps: 0.30711461559985764
	training loss at 200 steps: 0.6389164129213896
	training loss at 300 steps: 0.9215972188103478
	training loss at 400 steps: 1.1862075325771002
	training loss at 500 steps: 1.4578487670078175
	training loss at 600 steps: 1.74123178255104
	training loss at 700 steps: 2.023522792078438
	training loss at 800 steps: 2.2854069331515348
	training loss at 900 steps: 2.55550852631859
	training loss at 1000 steps: 2.8145433731988305
	training loss at 1100 steps: 3.099107464440749
	training loss at 1200 steps: 3.392618668382056
	training loss at 1300 steps: 3.672531946649542
	training loss at 1400 steps: 3.9728462357306853
	training loss at 1500 steps: 4.350571511895396
	training loss at 1600 steps: 4.813982168503571
	training loss at 1700 steps: 5.14598158269655
	training loss at 1800 steps: 5.443369860833627
	training loss at 1900 steps: 5.74873315768491
	training loss at 2000 steps: 6.049915975978365
	training loss at 2100 steps: 6.331893936105189
	training loss at 2200 steps: 6.634757714826264
	training loss at 2300 steps: 6.927797895288677
	training loss at 2400 steps: 7.25289328591316
	training loss at 2500 steps: 7.634488401148701
	training loss at 2600 steps: 7.9488292956666555
	training loss at 2700 steps: 8.233533155333134
	training loss at 2800 steps: 8.531988052200177
	training loss at 2900 steps: 8.825683961840696
	training loss at 3000 steps: 9.105169153714087
	training loss at 3100 steps: 9.4278300809965
	training loss at 3200 steps: 9.739423109247582
	training loss at 3300 steps: 10.05442421061889
	training loss at 3400 steps: 10.39265302336571
	training loss at 3500 steps: 10.727456337343028
	training loss at 3600 steps: 11.044680238039291
	training loss at 3700 steps: 11.39478639447043
	training loss at 3800 steps: 11.739087648536952
	training loss at 3900 steps: 12.099941226224473
	training loss at 4000 steps: 12.412503880135773
	training loss at 4100 steps: 12.71666813216143
	training loss at 4200 steps: 13.083065970444295
	training loss at 4300 steps: 13.39430540928879
	training loss at 4400 steps: 13.69299787228374
	training loss at 4500 steps: 13.993681743137131
	training loss at 4600 steps: 14.340974769736931
	training loss at 4700 steps: 14.666287860112789
	training loss at 4800 steps: 14.972706940032367
	Training loss for the epoch: 15.18935192918434
	Training accuracy for epoch: 0.9898137228023636
	Validation loss: 177.9312089709565
Epoch 18:
	training loss at 0 steps: 0.005982854403555393
	training loss at 100 steps: 0.2572799052504706
	training loss at 200 steps: 0.4841966468738974
	training loss at 300 steps: 0.7018321400755667
	training loss at 400 steps: 0.9638585522261565
	training loss at 500 steps: 1.2201626707610558
	training loss at 600 steps: 1.4459514059926732
	training loss at 700 steps: 1.6850049278218648
	training loss at 800 steps: 1.9185926622376428
	training loss at 900 steps: 2.1798822922210093
	training loss at 1000 steps: 2.4245459960293374
	training loss at 1100 steps: 2.6795309830959013
	training loss at 1200 steps: 2.9483548105381487
	training loss at 1300 steps: 3.212787337604823
	training loss at 1400 steps: 3.4658705301881128
	training loss at 1500 steps: 3.7426956523813715
	training loss at 1600 steps: 3.9957488699837995
	training loss at 1700 steps: 4.2616832366875315
	training loss at 1800 steps: 4.501841494140535
	training loss at 1900 steps: 4.7828213121283625
	training loss at 2000 steps: 5.074874732938042
	training loss at 2100 steps: 5.324912711450452
	training loss at 2200 steps: 5.604491973743279
	training loss at 2300 steps: 5.892493709496193
	training loss at 2400 steps: 6.12584360073015
	training loss at 2500 steps: 6.365578259148606
	training loss at 2600 steps: 6.659137764170737
	training loss at 2700 steps: 6.927160892792017
	training loss at 2800 steps: 7.212225131323066
	training loss at 2900 steps: 7.470585405037127
	training loss at 3000 steps: 7.7681090961705195
	training loss at 3100 steps: 8.057994254180812
	training loss at 3200 steps: 8.308622647004086
	training loss at 3300 steps: 8.59911522033508
	training loss at 3400 steps: 8.947357977478532
	training loss at 3500 steps: 9.232942479357007
	training loss at 3600 steps: 9.516993388209812
	training loss at 3700 steps: 9.776141419755731
	training loss at 3800 steps: 10.018766937788314
	training loss at 3900 steps: 10.345510651659424
	training loss at 4000 steps: 10.601443501651374
	training loss at 4100 steps: 10.87926224092007
	training loss at 4200 steps: 11.165155234393751
	training loss at 4300 steps: 11.454540208051185
	training loss at 4400 steps: 11.756479628074885
	training loss at 4500 steps: 12.031423133168573
	training loss at 4600 steps: 12.364407530996687
	training loss at 4700 steps: 12.645768933751242
	training loss at 4800 steps: 12.967144048521732
	Training loss for the epoch: 13.159451517316484
	Training accuracy for epoch: 0.9913215071387824
	Validation loss: 183.753913378343
Epoch 19:
	training loss at 0 steps: 0.0016952264122664928
	training loss at 100 steps: 0.18261989118764177
	training loss at 200 steps: 0.4055673725379165
	training loss at 300 steps: 0.6077999634799198
	training loss at 400 steps: 0.8263428824793664
	training loss at 500 steps: 1.0432735546710319
	training loss at 600 steps: 1.2537233606781228
	training loss at 700 steps: 1.4382281923681148
	training loss at 800 steps: 1.6673059846580145
	training loss at 900 steps: 1.9509510088100797
	training loss at 1000 steps: 2.161170908715576
	training loss at 1100 steps: 2.392083442056901
	training loss at 1200 steps: 2.621592417752254
	training loss at 1300 steps: 2.9620225555991055
	training loss at 1400 steps: 3.2196695450984407
	training loss at 1500 steps: 3.4411338097124826
	training loss at 1600 steps: 3.645649346188293
	training loss at 1700 steps: 3.862572654034011
	training loss at 1800 steps: 4.104483587565483
	training loss at 1900 steps: 4.358416013463284
	training loss at 2000 steps: 4.603351736397599
	training loss at 2100 steps: 4.903332906425931
	training loss at 2200 steps: 5.109704706148477
	training loss at 2300 steps: 5.348490362064695
	training loss at 2400 steps: 5.556651508764844
	training loss at 2500 steps: 5.789521895329017
	training loss at 2600 steps: 6.030642146792161
	training loss at 2700 steps: 6.241341727591134
	training loss at 2800 steps: 6.482578646415277
	training loss at 2900 steps: 6.72561628002768
	training loss at 3000 steps: 7.019110670875307
	training loss at 3100 steps: 7.264004429884153
	training loss at 3200 steps: 7.495229121583179
	training loss at 3300 steps: 7.776588018414259
	training loss at 3400 steps: 8.026297161120965
	training loss at 3500 steps: 8.326610355316006
	training loss at 3600 steps: 8.61049039741738
	training loss at 3700 steps: 8.933086254894079
	training loss at 3800 steps: 9.205091427180378
	training loss at 3900 steps: 9.447525432142356
	training loss at 4000 steps: 9.686943706603415
	training loss at 4100 steps: 9.906128989487115
	training loss at 4200 steps: 10.125390888022594
	training loss at 4300 steps: 10.342209371161516
	training loss at 4400 steps: 10.580509538825936
	training loss at 4500 steps: 10.823282229632241
	training loss at 4600 steps: 11.06313849342223
	training loss at 4700 steps: 11.341928939398713
	training loss at 4800 steps: 11.591551527124466
	Training loss for the epoch: 11.812092482554363
	Training accuracy for epoch: 0.9923146402645976
	Validation loss: 179.7751619326882
Epoch 20:
	training loss at 0 steps: 0.0003797786484938115
	training loss at 100 steps: 0.3178656521922676
	training loss at 200 steps: 0.6500891568139195
	training loss at 300 steps: 0.8510285409720382
	training loss at 400 steps: 1.1280025613232283
	training loss at 500 steps: 1.4391129870637087
	training loss at 600 steps: 1.7683791848103283
	training loss at 700 steps: 2.011757133928768
	training loss at 800 steps: 2.229443830678065
	training loss at 900 steps: 2.416644401178928
	training loss at 1000 steps: 2.5855337745451834
	training loss at 1100 steps: 2.804597757902229
	training loss at 1200 steps: 3.0153357386152493
	training loss at 1300 steps: 3.2293166964082047
	training loss at 1400 steps: 3.4544484847137937
	training loss at 1500 steps: 3.6943135051697027
	training loss at 1600 steps: 3.938798504823353
	training loss at 1700 steps: 4.173481903912034
	training loss at 1800 steps: 4.377649058893439
	training loss at 1900 steps: 4.543376628484111
	training loss at 2000 steps: 4.746471830134396
	training loss at 2100 steps: 4.9408337530330755
	training loss at 2200 steps: 5.158632081234828
	training loss at 2300 steps: 5.391899339912925
	training loss at 2400 steps: 5.634010053850943
	training loss at 2500 steps: 5.865291382509895
	training loss at 2600 steps: 6.149952349765954
	training loss at 2700 steps: 6.344910964409792
	training loss at 2800 steps: 6.564605688803567
	training loss at 2900 steps: 6.771115073341207
	training loss at 3000 steps: 7.018857724633563
	training loss at 3100 steps: 7.201811053764686
	training loss at 3200 steps: 7.415037941380433
	training loss at 3300 steps: 7.641230504155828
	training loss at 3400 steps: 7.899114105664921
	training loss at 3500 steps: 8.16270170957796
	training loss at 3600 steps: 8.421487224291923
	training loss at 3700 steps: 8.684280840057909
	training loss at 3800 steps: 8.92383494551541
	training loss at 3900 steps: 9.182364107717149
	training loss at 4000 steps: 9.375097746811662
	training loss at 4100 steps: 9.592827720505738
	training loss at 4200 steps: 9.795506751612265
	training loss at 4300 steps: 10.024177259063435
	training loss at 4400 steps: 10.268830042648915
	training loss at 4500 steps: 10.476689494822494
	training loss at 4600 steps: 10.702567799820827
	training loss at 4700 steps: 10.936271757735085
	training loss at 4800 steps: 11.181764085489704
	Training loss for the epoch: 11.356818313853182
	Training accuracy for epoch: 0.9927840730447747
	Validation loss: 195.3143572807312
Epoch 21:
	training loss at 0 steps: 0.00030138762667775154
	training loss at 100 steps: 0.18823876249371096
	training loss at 200 steps: 0.37077481411688495
	training loss at 300 steps: 0.5415403859406069
	training loss at 400 steps: 0.7725608692453534
	training loss at 500 steps: 1.003370517926669
	training loss at 600 steps: 1.2141087663949293
	training loss at 700 steps: 1.3919098642400058
	training loss at 800 steps: 1.5553279828709492
	training loss at 900 steps: 1.731611914397945
	training loss at 1000 steps: 1.8966527555494395
	training loss at 1100 steps: 2.109292925648333
	training loss at 1200 steps: 2.2799620284931734
	training loss at 1300 steps: 2.481496390144457
	training loss at 1400 steps: 2.6582455711904913
	training loss at 1500 steps: 2.810789986731834
	training loss at 1600 steps: 2.9701455810100015
	training loss at 1700 steps: 3.2053357843360573
	training loss at 1800 steps: 3.415562424601376
	training loss at 1900 steps: 3.6342233441519056
	training loss at 2000 steps: 3.8449833554095676
	training loss at 2100 steps: 4.077561251824591
	training loss at 2200 steps: 4.265157027561145
	training loss at 2300 steps: 4.462546916793144
	training loss at 2400 steps: 4.6804103420709
	training loss at 2500 steps: 4.890260281352312
	training loss at 2600 steps: 5.074029128152688
	training loss at 2700 steps: 5.3176010901934205
	training loss at 2800 steps: 5.547755817455254
	training loss at 2900 steps: 5.750418065666963
	training loss at 3000 steps: 5.933341934918644
	training loss at 3100 steps: 6.139529479796693
	training loss at 3200 steps: 6.3354225891944225
	training loss at 3300 steps: 6.543929441582804
	training loss at 3400 steps: 6.743518933899395
	training loss at 3500 steps: 6.937905447275625
	training loss at 3600 steps: 7.098714394023773
	training loss at 3700 steps: 7.253742235394384
	training loss at 3800 steps: 7.459394664650972
	training loss at 3900 steps: 7.6643892708762
	training loss at 4000 steps: 7.914560999908645
	training loss at 4100 steps: 8.125404667180192
	training loss at 4200 steps: 8.364269472212982
	training loss at 4300 steps: 8.582360922393491
	training loss at 4400 steps: 8.784831983552067
	training loss at 4500 steps: 9.00657032878189
	training loss at 4600 steps: 9.227643788390196
	training loss at 4700 steps: 9.433295249828006
	training loss at 4800 steps: 9.624831006234672
	Training loss for the epoch: 9.75205114700475
	Training accuracy for epoch: 0.9935801919634746
	Validation loss: 196.45765483286232
Epoch 22:
	training loss at 0 steps: 0.0009007310727611184
	training loss at 100 steps: 0.13489411865157308
	training loss at 200 steps: 0.2814915694107185
	training loss at 300 steps: 0.469692362297792
	training loss at 400 steps: 0.6250194199819816
	training loss at 500 steps: 0.7629493238273426
	training loss at 600 steps: 0.9393350165482843
	training loss at 700 steps: 1.093774992536055
	training loss at 800 steps: 1.2823257560521597
	training loss at 900 steps: 1.4767177775065647
	training loss at 1000 steps: 1.6260188543092227
	training loss at 1100 steps: 1.8135561586095719
	training loss at 1200 steps: 2.0017190276266774
	training loss at 1300 steps: 2.1365299072058406
	training loss at 1400 steps: 2.3236929670711106
	training loss at 1500 steps: 2.515028837897262
	training loss at 1600 steps: 2.691902866550663
	training loss at 1700 steps: 2.870349486664054
	training loss at 1800 steps: 3.037311504060199
	training loss at 1900 steps: 3.209529909352568
	training loss at 2000 steps: 3.3823338195725228
	training loss at 2100 steps: 3.543414286694315
	training loss at 2200 steps: 3.7118261162177077
	training loss at 2300 steps: 3.893578139127385
	training loss at 2400 steps: 4.049695767874255
	training loss at 2500 steps: 4.291022014142982
	training loss at 2600 steps: 4.469122790346773
	training loss at 2700 steps: 4.638422536995677
	training loss at 2800 steps: 4.822265177460395
	training loss at 2900 steps: 4.9700518324871155
	training loss at 3000 steps: 5.132875128642809
	training loss at 3100 steps: 5.306067028069265
	training loss at 3200 steps: 5.487390798757588
	training loss at 3300 steps: 5.65672612698927
	training loss at 3400 steps: 5.877148517637579
	training loss at 3500 steps: 6.161803112697271
	training loss at 3600 steps: 6.384808661777242
	training loss at 3700 steps: 6.54532797036245
	training loss at 3800 steps: 6.857997838921619
	training loss at 3900 steps: 7.060289851401649
	training loss at 4000 steps: 7.257128633603315
	training loss at 4100 steps: 7.446048080150831
	training loss at 4200 steps: 7.626254738600437
	training loss at 4300 steps: 7.84318596846515
	training loss at 4400 steps: 8.029389786629508
	training loss at 4500 steps: 8.266446195958451
	training loss at 4600 steps: 8.406637541943383
	training loss at 4700 steps: 8.60397055971498
	training loss at 4800 steps: 8.81194080732621
	Training loss for the epoch: 8.934942296656118
	Training accuracy for epoch: 0.9943433808696546
	Validation loss: 195.2241404633969
Epoch 23:
	training loss at 0 steps: 0.0003092898987233639
	training loss at 100 steps: 0.11820341852944694
	training loss at 200 steps: 0.2649820827136864
	training loss at 300 steps: 0.4104252071410883
	training loss at 400 steps: 0.5575836582866032
	training loss at 500 steps: 1.0695002109132474
	training loss at 600 steps: 1.2510209055035375
	training loss at 700 steps: 1.4538007341470802
	training loss at 800 steps: 1.620970954405493
	training loss at 900 steps: 1.8034129394218326
	training loss at 1000 steps: 1.9793133746425156
	training loss at 1100 steps: 2.1513100194133585
	training loss at 1200 steps: 2.3792597144383762
	training loss at 1300 steps: 2.5727553268297925
	training loss at 1400 steps: 2.766288858543703
	training loss at 1500 steps: 2.9387920779900014
	training loss at 1600 steps: 3.070587578076811
	training loss at 1700 steps: 3.237129173048743
	training loss at 1800 steps: 3.3531025536285597
	training loss at 1900 steps: 3.51219706035954
	training loss at 2000 steps: 3.6796056125331233
	training loss at 2100 steps: 3.8106393326688703
	training loss at 2200 steps: 3.9825959464014886
	training loss at 2300 steps: 4.136246503772782
	training loss at 2400 steps: 4.314949256624459
	training loss at 2500 steps: 4.482538390975606
	training loss at 2600 steps: 4.642435392937841
	training loss at 2700 steps: 4.793779561170595
	training loss at 2800 steps: 4.9692565178283985
	training loss at 2900 steps: 5.099280705400815
	training loss at 3000 steps: 5.244599388433926
	training loss at 3100 steps: 5.431332220616241
	training loss at 3200 steps: 5.600166431164325
	training loss at 3300 steps: 5.757060188099786
	training loss at 3400 steps: 5.934111186996233
	training loss at 3500 steps: 6.102570385673971
	training loss at 3600 steps: 6.2476679255905765
	training loss at 3700 steps: 6.403419683423635
	training loss at 3800 steps: 6.573522812881492
	training loss at 3900 steps: 6.72360560519337
	training loss at 4000 steps: 6.905131417695884
	training loss at 4100 steps: 7.06516081562404
	training loss at 4200 steps: 7.218247852559216
	training loss at 4300 steps: 7.4153661315740464
	training loss at 4400 steps: 7.627578422785518
	training loss at 4500 steps: 7.846556216389217
	training loss at 4600 steps: 7.994709234166294
	training loss at 4700 steps: 8.154070791746562
	training loss at 4800 steps: 8.352655290198527
	Training loss for the epoch: 8.498816407600316
	Training accuracy for epoch: 0.9946208406305672
	Validation loss: 201.65099919214845
Epoch 24:
	training loss at 0 steps: 0.00020019174553453922
	training loss at 100 steps: 0.16537762135158118
	training loss at 200 steps: 0.29575143370857404
	training loss at 300 steps: 0.41921807380094833
	training loss at 400 steps: 0.582235585070066
	training loss at 500 steps: 0.7788389393549551
	training loss at 600 steps: 0.9040463514479598
	training loss at 700 steps: 1.0313676799319182
	training loss at 800 steps: 1.1823180507976758
	training loss at 900 steps: 1.3093353417893923
	training loss at 1000 steps: 1.4154366920552093
	training loss at 1100 steps: 1.5421433857513875
	training loss at 1200 steps: 1.675819419876916
	training loss at 1300 steps: 1.821647936397767
	training loss at 1400 steps: 2.0298840204263797
	training loss at 1500 steps: 2.2002887004414333
	training loss at 1600 steps: 2.364220262526487
	training loss at 1700 steps: 2.526518045753164
	training loss at 1800 steps: 2.7355595183084915
	training loss at 1900 steps: 2.8794400429665075
	training loss at 2000 steps: 3.0269013404481484
	training loss at 2100 steps: 3.177997679996679
	training loss at 2200 steps: 3.3144050116475228
	training loss at 2300 steps: 3.469076718847191
	training loss at 2400 steps: 3.6190450833705654
	training loss at 2500 steps: 3.754164059316736
	training loss at 2600 steps: 3.927031634395007
	training loss at 2700 steps: 4.085510357786461
	training loss at 2800 steps: 4.3111793020812
	training loss at 2900 steps: 4.512611529965852
	training loss at 3000 steps: 4.641334116431153
	training loss at 3100 steps: 4.786077118998492
	training loss at 3200 steps: 4.921544469073979
	training loss at 3300 steps: 5.08332542560629
	training loss at 3400 steps: 5.228626533905754
	training loss at 3500 steps: 5.398141936580487
	training loss at 3600 steps: 5.610336514080245
	training loss at 3700 steps: 5.763539825289172
	training loss at 3800 steps: 5.969689147138979
	training loss at 3900 steps: 6.1766243222732555
	training loss at 4000 steps: 6.342582488432072
	training loss at 4100 steps: 6.491526623794471
	training loss at 4200 steps: 6.605780992141717
	training loss at 4300 steps: 6.758669705299781
	training loss at 4400 steps: 6.935755939530736
	training loss at 4500 steps: 7.099473463613776
	training loss at 4600 steps: 7.246952091615185
	training loss at 4700 steps: 7.399174166854664
	training loss at 4800 steps: 7.579596200929245
	Training loss for the epoch: 7.679663202245592
	Training accuracy for epoch: 0.9951703314993816
	Validation loss: 202.8785665202886
Epoch 25:
	training loss at 0 steps: 0.00020398256310727447
	training loss at 100 steps: 0.13897012203233317
	training loss at 200 steps: 0.24890136376234295
	training loss at 300 steps: 0.37807021429944143
	training loss at 400 steps: 0.4971021902956636
	training loss at 500 steps: 0.6498080074261452
	training loss at 600 steps: 0.8030041076845009
	training loss at 700 steps: 1.0220887276800568
	training loss at 800 steps: 1.1634227018685124
	training loss at 900 steps: 1.3143487524639568
	training loss at 1000 steps: 1.4495537828688612
	training loss at 1100 steps: 1.5993290259166315
	training loss at 1200 steps: 1.738317865276258
	training loss at 1300 steps: 1.8509670581915998
	training loss at 1400 steps: 2.0060169176194904
	training loss at 1500 steps: 2.1909318352099945
	training loss at 1600 steps: 2.3547431003444217
	training loss at 1700 steps: 2.4888465643616655
	training loss at 1800 steps: 2.609207676132428
	training loss at 1900 steps: 2.7310768665029173
	training loss at 2000 steps: 2.95146521535753
	training loss at 2100 steps: 3.094839132134439
	training loss at 2200 steps: 3.2211519844076975
	training loss at 2300 steps: 3.385610606159389
	training loss at 2400 steps: 3.5203704124705837
	training loss at 2500 steps: 3.6956617024061416
	training loss at 2600 steps: 3.8070272166542054
	training loss at 2700 steps: 3.986567962506342
	training loss at 2800 steps: 4.149960846453723
	training loss at 2900 steps: 4.3308159308171525
	training loss at 3000 steps: 4.485782810795172
	training loss at 3100 steps: 4.626126469210249
	training loss at 3200 steps: 4.761397614401176
	training loss at 3300 steps: 4.909007068726169
	training loss at 3400 steps: 5.062796124580018
	training loss at 3500 steps: 5.206618893777886
	training loss at 3600 steps: 5.386092030744294
	training loss at 3700 steps: 5.55904651077708
	training loss at 3800 steps: 5.695022607823375
	training loss at 3900 steps: 5.8572703802319666
	training loss at 4000 steps: 5.973747548777283
	training loss at 4100 steps: 6.14206937300969
	training loss at 4200 steps: 6.27162609257266
	training loss at 4300 steps: 6.404097070098032
	training loss at 4400 steps: 6.550147368555372
	training loss at 4500 steps: 6.689142577921302
	training loss at 4600 steps: 6.860147445153416
	training loss at 4700 steps: 6.994867916026124
	training loss at 4800 steps: 7.138705468201806
	Training loss for the epoch: 7.236973168477562
	Training accuracy for epoch: 0.9953714386469863
	Validation loss: 204.11248090863228
Epoch 26:
	training loss at 0 steps: 0.000820427609141916
	training loss at 100 steps: 0.13967950396545348
	training loss at 200 steps: 0.27388802152381686
	training loss at 300 steps: 0.42343207497651747
	training loss at 400 steps: 0.5560397804474633
	training loss at 500 steps: 0.6648467254126444
	training loss at 600 steps: 0.7842776276993391
	training loss at 700 steps: 0.9175278555667319
	training loss at 800 steps: 1.0187725507457799
	training loss at 900 steps: 1.1481984866741186
	training loss at 1000 steps: 1.2733029876371802
	training loss at 1100 steps: 1.4271958857034406
	training loss at 1200 steps: 1.5483991217452058
	training loss at 1300 steps: 1.6498431262707527
	training loss at 1400 steps: 1.7923850694187422
	training loss at 1500 steps: 1.929260188144326
	training loss at 1600 steps: 2.068335348274559
	training loss at 1700 steps: 2.1713320943863437
	training loss at 1800 steps: 2.2964663726843355
	training loss at 1900 steps: 2.4743280725624572
	training loss at 2000 steps: 2.5799913488799575
	training loss at 2100 steps: 2.716919144032545
	training loss at 2200 steps: 2.866216635696219
	training loss at 2300 steps: 3.014027707603418
	training loss at 2400 steps: 3.1615191019855047
	training loss at 2500 steps: 3.4043839099440447
	training loss at 2600 steps: 3.5649330027781616
	training loss at 2700 steps: 3.722948983923743
	training loss at 2800 steps: 3.9129436332959813
	training loss at 2900 steps: 4.052116263849712
	training loss at 3000 steps: 4.1997617848455775
	training loss at 3100 steps: 4.340844964549433
	training loss at 3200 steps: 4.487832619720393
	training loss at 3300 steps: 4.710534110700792
	training loss at 3400 steps: 4.881528818778861
	training loss at 3500 steps: 5.008901908632652
	training loss at 3600 steps: 5.200401940816846
	training loss at 3700 steps: 5.330121523534217
	training loss at 3800 steps: 5.52492298627476
	training loss at 3900 steps: 5.6947646922199056
	training loss at 4000 steps: 5.857994549052819
	training loss at 4100 steps: 5.972588719458145
	training loss at 4200 steps: 6.133219582145102
	training loss at 4300 steps: 6.254395629912324
	training loss at 4400 steps: 6.400055724887352
	training loss at 4500 steps: 6.534653904440347
	training loss at 4600 steps: 6.662142614703043
	training loss at 4700 steps: 6.801856636899174
	training loss at 4800 steps: 6.955053002526256
	Training loss for the epoch: 7.06438807025188
	Training accuracy for epoch: 0.9955550078173178
	Validation loss: 199.3222176246345
Epoch 27:
	training loss at 0 steps: 0.00027562151080928743
	training loss at 100 steps: 0.11198167976181139
	training loss at 200 steps: 0.22372474130315823
	training loss at 300 steps: 0.3331111663428601
	training loss at 400 steps: 0.4492740737341592
	training loss at 500 steps: 0.554576244888267
	training loss at 600 steps: 0.6476013841956956
	training loss at 700 steps: 0.75710283006174
	training loss at 800 steps: 0.85675104386155
	training loss at 900 steps: 0.9803589585144437
	training loss at 1000 steps: 1.0913119109400213
	training loss at 1100 steps: 1.2256284436853093
	training loss at 1200 steps: 1.3643820172901542
	training loss at 1300 steps: 1.6710906824800986
	training loss at 1400 steps: 1.83696610637071
	training loss at 1500 steps: 1.9646569806436673
	training loss at 1600 steps: 2.084370618057619
	training loss at 1700 steps: 2.2211760565196528
	training loss at 1800 steps: 2.3684461650263984
	training loss at 1900 steps: 2.5237328783150588
	training loss at 2000 steps: 2.654522897541028
	training loss at 2100 steps: 2.772021876182407
	training loss at 2200 steps: 2.9137123440414143
	training loss at 2300 steps: 3.0368467284351937
	training loss at 2400 steps: 3.158364749064276
	training loss at 2500 steps: 3.2678802106565854
	training loss at 2600 steps: 3.389989272098319
	training loss at 2700 steps: 3.5143081566529872
	training loss at 2800 steps: 3.6520573631532898
	training loss at 2900 steps: 3.7489406769545894
	training loss at 3000 steps: 3.887959872952706
	training loss at 3100 steps: 4.045021704891042
	training loss at 3200 steps: 4.17114547563142
	training loss at 3300 steps: 4.282439878150399
	training loss at 3400 steps: 4.415657373881913
	training loss at 3500 steps: 4.524837511738951
	training loss at 3600 steps: 4.645924941638441
	training loss at 3700 steps: 4.756966403439947
	training loss at 3800 steps: 4.862955699156373
	training loss at 3900 steps: 4.988110085599146
	training loss at 4000 steps: 5.121125995213333
	training loss at 4100 steps: 5.227520340426054
	training loss at 4200 steps: 5.370376683264112
	training loss at 4300 steps: 5.518480728496797
	training loss at 4400 steps: 5.682974073657533
	training loss at 4500 steps: 5.791937041785786
	training loss at 4600 steps: 5.917233255260726
	training loss at 4700 steps: 6.023726132358206
	training loss at 4800 steps: 6.183575324355843
	Training loss for the epoch: 6.268928725983642
	Training accuracy for epoch: 0.9960419417944252
	Validation loss: 203.5899945097044
Epoch 28:
	training loss at 0 steps: 0.0028458863962441683
	training loss at 100 steps: 0.12118363470290205
	training loss at 200 steps: 0.2182097400946077
	training loss at 300 steps: 0.3212991562213574
	training loss at 400 steps: 0.40932348670503416
	training loss at 500 steps: 0.5386724335030522
	training loss at 600 steps: 0.6381422929152905
	training loss at 700 steps: 0.7508519979492121
	training loss at 800 steps: 0.8388191474641644
	training loss at 900 steps: 0.9778641883585806
	training loss at 1000 steps: 1.0805801652149967
	training loss at 1100 steps: 1.192607211860377
	training loss at 1200 steps: 1.2994413089818408
	training loss at 1300 steps: 1.6051982122844493
	training loss at 1400 steps: 1.7538538527187484
	training loss at 1500 steps: 1.944111812981646
	training loss at 1600 steps: 2.063146435206363
	training loss at 1700 steps: 2.1705888206165582
	training loss at 1800 steps: 2.2812689295410564
	training loss at 1900 steps: 2.466332728105499
	training loss at 2000 steps: 2.5784077015891853
	training loss at 2100 steps: 2.754961231481957
	training loss at 2200 steps: 2.8834637026579912
	training loss at 2300 steps: 3.042545189577595
	training loss at 2400 steps: 3.161157236238523
	training loss at 2500 steps: 3.2736333329144145
	training loss at 2600 steps: 3.3870383620728717
	training loss at 2700 steps: 3.5043734905616475
	training loss at 2800 steps: 3.6432490846477776
	training loss at 2900 steps: 3.7415802743948916
	training loss at 3000 steps: 3.8504540453418485
	training loss at 3100 steps: 3.974082088598607
	training loss at 3200 steps: 4.090177739052251
	training loss at 3300 steps: 4.220740047114759
	training loss at 3400 steps: 4.335992726263157
	training loss at 3500 steps: 4.45548599449512
	training loss at 3600 steps: 4.55486452922969
	training loss at 3700 steps: 4.6723314273481265
	training loss at 3800 steps: 4.788903727880552
	training loss at 3900 steps: 4.916515251796682
	training loss at 4000 steps: 5.294313847544345
	training loss at 4100 steps: 5.473177506918091
	training loss at 4200 steps: 5.588653632365549
	training loss at 4300 steps: 5.69608516105427
	training loss at 4400 steps: 5.789489798998602
	training loss at 4500 steps: 5.930829714521224
	training loss at 4600 steps: 6.037411415581573
	training loss at 4700 steps: 6.159905044477
	training loss at 4800 steps: 6.327348447803615
	Training loss for the epoch: 6.512997643169456
	Training accuracy for epoch: 0.9961248655212283
	Validation loss: 206.19397544441745
Epoch 29:
	training loss at 0 steps: 0.0003134913567919284
	training loss at 100 steps: 0.2951042937420425
	training loss at 200 steps: 0.404432731516863
	training loss at 300 steps: 0.5012624805949599
	training loss at 400 steps: 0.6266091276193038
	training loss at 500 steps: 0.7389439531743847
	training loss at 600 steps: 0.8941701254534564
	training loss at 700 steps: 1.0074405108325664
	training loss at 800 steps: 1.1281219149286699
	training loss at 900 steps: 1.2310295194356513
	training loss at 1000 steps: 1.3328918948982391
	training loss at 1100 steps: 1.4146376423059337
	training loss at 1200 steps: 1.556297184716641
	training loss at 1300 steps: 1.6827488282506238
	training loss at 1400 steps: 1.8226563385687768
	training loss at 1500 steps: 1.9336879666661844
	training loss at 1600 steps: 2.0299982120559434
	training loss at 1700 steps: 2.1192836949194316
	training loss at 1800 steps: 2.212549426525584
	training loss at 1900 steps: 2.31195277835468
	training loss at 2000 steps: 2.4080698760917585
	training loss at 2100 steps: 2.5189983347390807
	training loss at 2200 steps: 2.6448282268920593
	training loss at 2300 steps: 2.9133590889141487
	training loss at 2400 steps: 3.061636829890631
	training loss at 2500 steps: 3.1756194349600264
	training loss at 2600 steps: 3.293055727828687
	training loss at 2700 steps: 3.398531630760772
	training loss at 2800 steps: 3.498760031097845
	training loss at 2900 steps: 3.598544152974682
	training loss at 3000 steps: 3.7215329898017444
	training loss at 3100 steps: 3.8897741020564354
	training loss at 3200 steps: 3.9855720378336628
	training loss at 3300 steps: 4.0879372970057375
	training loss at 3400 steps: 4.191454181164772
	training loss at 3500 steps: 4.293158229528672
	training loss at 3600 steps: 4.4081873818713575
	training loss at 3700 steps: 4.577396692799994
	training loss at 3800 steps: 4.679280789171571
	training loss at 3900 steps: 4.786456436787375
	training loss at 4000 steps: 4.893835533721358
	training loss at 4100 steps: 5.00441562542801
	training loss at 4200 steps: 5.098098682183263
	training loss at 4300 steps: 5.232815042543734
	training loss at 4400 steps: 5.347513301925574
	training loss at 4500 steps: 5.442512365490984
	training loss at 4600 steps: 5.548961814631184
	training loss at 4700 steps: 5.6891866988480615
	training loss at 4800 steps: 5.8476577014753275
	Training loss for the epoch: 5.972184716652919
	Training accuracy for epoch: 0.9963654203528545
	Validation loss: 203.17693339195102
Epoch 30:
	training loss at 0 steps: 6.260807276703417e-05
	training loss at 100 steps: 0.10608862762546778
	training loss at 200 steps: 0.20715049259979423
	training loss at 300 steps: 0.31392683879403194
	training loss at 400 steps: 0.39853045810832555
	training loss at 500 steps: 0.5080069841669683
	training loss at 600 steps: 0.623291065498961
	training loss at 700 steps: 0.7321946823403778
	training loss at 800 steps: 0.9672442604096432
	training loss at 900 steps: 1.0598054323163524
	training loss at 1000 steps: 1.1455774922396813
	training loss at 1100 steps: 1.293194016740017
	training loss at 1200 steps: 1.4277718217526854
	training loss at 1300 steps: 1.5693336164331413
	training loss at 1400 steps: 1.6651788336494064
	training loss at 1500 steps: 1.7572364098741673
	training loss at 1600 steps: 1.848008868116267
	training loss at 1700 steps: 1.9353545722142371
	training loss at 1800 steps: 2.01765214072293
	training loss at 1900 steps: 2.1141079036042356
	training loss at 2000 steps: 2.203773993971481
	training loss at 2100 steps: 2.2945185012795264
	training loss at 2200 steps: 2.401552543953585
	training loss at 2300 steps: 2.5926458231697325
	training loss at 2400 steps: 2.70738698589048
	training loss at 2500 steps: 2.8321042294037397
	training loss at 2600 steps: 2.928077318348187
	training loss at 2700 steps: 3.004187529258161
	training loss at 2800 steps: 3.095607159278188
	training loss at 2900 steps: 3.1717480736306243
	training loss at 3000 steps: 3.2385904313259744
	training loss at 3100 steps: 3.3104421563120923
	training loss at 3200 steps: 3.390011846836387
	training loss at 3300 steps: 3.4770828868977333
	training loss at 3400 steps: 3.561896389936919
	training loss at 3500 steps: 3.6504038912416945
	training loss at 3600 steps: 3.8566116105075707
	training loss at 3700 steps: 3.9452440849609047
	training loss at 3800 steps: 4.021252667439512
	training loss at 3900 steps: 4.1528224404255525
	training loss at 4000 steps: 4.24340774207576
	training loss at 4100 steps: 4.359168613845213
	training loss at 4200 steps: 4.47897619629839
	training loss at 4300 steps: 4.584739876889216
	training loss at 4400 steps: 4.67169320437506
	training loss at 4500 steps: 4.841046654630645
	training loss at 4600 steps: 4.941246343831608
	training loss at 4700 steps: 5.063189340577537
	training loss at 4800 steps: 5.196540162980909
	Training loss for the epoch: 5.27824659123371
	Training accuracy for epoch: 0.9968708171190611
	Validation loss: 207.18905742280185
Generating prediction file for MedMentions using early stopped model
	 Test accuracy: 0.8539680792667408
Generating prediction file for MedMentions using 30 epoch model
	 Test accuracy: 0.8526250338003843

Performance on MedMentions using early stopped model

60892 53800
Splits | Mem: 46442, Syn: 16104, Zero: 3004

--Evaluation--
Overall 65.6	68.9	67.2
Mem 70.4
Syn 65.4
Con 65.6

Performacne on MedMentions using 30 epoch model

60892 53800
Splits | Mem: 46442, Syn: 16104, Zero: 3004

--Evaluation--
Overall 65.0	67.6	66.3
Mem 68.9
Syn 64.5
Con 65.0
